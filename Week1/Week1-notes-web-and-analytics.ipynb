{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb3cc272",
   "metadata": {},
   "source": [
    "#  Web and Web Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba88fbf",
   "metadata": {},
   "source": [
    "## Scraping an html page (loading and searching it's contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6461cbd",
   "metadata": {},
   "source": [
    "* Local: saved in a file on your computer\n",
    "* Remote: somewhere on the web"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac056db",
   "metadata": {},
   "source": [
    "To fully understand this notebook, please open `example_html.html` file in another tab, and open it's `example_html.html`'s source code in a third tab (or even better: in browser's view > developer tools). You will see in a minute what is the exact address in that file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84395eee",
   "metadata": {},
   "source": [
    "For scraping, we need a few of different libraries, most notably Beautifulsoup. Let's first import these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b4567524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3404781d",
   "metadata": {},
   "source": [
    "We can simply enter a web page as a string and open it. Afterwards, BeautifulSoup converts it into a BeautifulSoup object which has many interesting functions and attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bcd481e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paste this url to your browser to see the demo website (copy the whole thing, together wioth the file:// part):\n",
      "file:////Users/caleb/Documents/Edinburgh/Msc Business Analytics/Web and Social Network Analytics/WebSNA-notes/Week1/example_html.html\n"
     ]
    }
   ],
   "source": [
    "# website address\n",
    "#page = 'http://www.uebs.ed.ac.uk'\n",
    "\n",
    "# open the url and store the website\n",
    "#website = urlopen(page)\n",
    "\n",
    "# for now we use a local file (os.getcwd() gets the Current Working Directory, aka. the folder you're in)\n",
    "file_url = \"file:///\"+os.getcwd()+\"/example_html.html\"\n",
    "website_source_code = urlopen(file_url)\n",
    "\n",
    "\n",
    "# in another tab: (open the example_html.html file directly in your browser to see how it will look like)\n",
    "# then in your browser, right click and select 'view source', or open developer tools to see the source\n",
    "print(\"Paste this url to your browser to see the demo website (copy the whole thing, together wioth the file:// part):\")\n",
    "print( file_url)\n",
    "\n",
    "# convert the website's content, for this a parser is needed. In this case a html parser\n",
    "soup = BeautifulSoup(website_source_code, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d13a6457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "\n",
      "<html>\n",
      "<head>\n",
      "<style>\n",
      ".hipster {\n",
      "\tbackground-color:black;\n",
      "\tcolor:red;\n",
      "\tpadding:22px;\n",
      "}\n",
      "</style>\n",
      "<script type=\"text/javascript\">\n",
      "  var numberOfClicks = 0;\n",
      "  function clickedButton()\n",
      "  {\n",
      "      numberOfClicks += 1;\n",
      "    document.getElementById(\"clickableButton\").text=\"GOOD JOB! You clicked me \"+numberOfClicks+\" times. If you reload the page I will go back to the original state :)\"; \n",
      "  }\n",
      "</script>\n",
      "</head>\n",
      "<body>\n",
      "<h1 title=\"A header\">Example for Media and Web Analytics</h1>\n",
      "<p>Here you typically see some text.\n",
      "Ocassionaly, an URL is present <a href=\"http://www.ed.ac.uk\">UoE</a>\n",
      "</p>\n",
      "<h1 title=\"A header\">Some other stuff</h1>\n",
      "<h2>3 Rows and 3 Columns:</h2>\n",
      "<table>\n",
      "<tr>\n",
      "<td>100</td>\n",
      "<td>200</td>\n",
      "<td>300</td>\n",
      "</tr>\n",
      "<tr id=\"middle_row\">\n",
      "<td>400</td>\n",
      "<td>500</td>\n",
      "<td>600</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>700</td>\n",
      "<td>800</td>\n",
      "<td>900</td>\n",
      "</tr>\n",
      "</table>\n",
      "<a href=\"#\" id=\"clickableButton\" onclick=\"clickedButton()\" target=\"none\">CLICK ME!</a>\n",
      "<div class=\"hipster\">\n",
      "<h2>A Dangerous-Looking Header</h2>\n",
      "<p>\n",
      "I look like a paragraph Kylo Ren could have written.\n",
      "</p>\n",
      "</div>\n",
      "<div class=\"hipster\">\n",
      "<h2>Another Dangerous-Looking Header</h2>\n",
      "<p>\n",
      "This one is not as scary.\n",
      "</p>\n",
      "</div>\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "# here's a complete html of the page, but it's easier to read if you open it's source using the url above\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "503b4ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete tag code:  <h1 title=\"A header\">Example for Media and Web Analytics</h1>\n",
      "Just the text in the tag:  Example for Media and Web Analytics\n",
      "Complete tag code:  <h1 title=\"A header\">Some other stuff</h1>\n",
      "Just the text in the tag:  Some other stuff\n"
     ]
    }
   ],
   "source": [
    "# .find_all retrieves all tags containing 'h1':\n",
    "h1Tags = soup.find_all('h1')\n",
    "for h1 in h1Tags:\n",
    "    print('Complete tag code: ', h1)\n",
    "    print(\"Just the text in the tag: \", h1.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43217984",
   "metadata": {},
   "source": [
    "It does not work with attributes of tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "929ee730",
   "metadata": {},
   "outputs": [],
   "source": [
    "titleTags = soup.find_all('title')\n",
    "for title in titleTags:\n",
    "    print('Complete tag code: ', title)\n",
    "    print(\"Just the text in the tag: \", title.text)\n",
    "    \n",
    "# nothing will be printed. there are no tags <title> </title> there"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4148c8c0",
   "metadata": {},
   "source": [
    "## Understanding the html is all about finding components you need:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccce23f0",
   "metadata": {},
   "source": [
    "* .find_all( ) will find all things that match criteria, in a list\n",
    "* .find( ) will find just the first item that mathes the criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d52022",
   "metadata": {},
   "source": [
    "You can use it on the whole website, like `a_table = soup.find(\"table\")` or on an element you found before `rows = a_table.find(\"tr\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e05760b",
   "metadata": {},
   "source": [
    "You can seek for types of tags, classes or ids\n",
    "* `soup.find(\"h1\")`, \n",
    "* `soup.find(id=\"main_navigation\")`,\n",
    "* `soup.find(class=\"warning_message\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51351d45",
   "metadata": {},
   "source": [
    "But it is very frequent to fetch an element by its unique id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "91709bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete tag code:  <tr id=\"middle_row\">\n",
      "<td>400</td>\n",
      "<td>500</td>\n",
      "<td>600</td>\n",
      "</tr>\n",
      "Just the text in the tag:  \n",
      "400\n",
      "500\n",
      "600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "middle_row = soup.find(id='middle_row')\n",
    "\n",
    "print('Complete tag code: ', middle_row)\n",
    "print(\"Just the text in the tag: \", middle_row.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf9b0b0",
   "metadata": {},
   "source": [
    "## Find children:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cca63b3",
   "metadata": {},
   "source": [
    "When, like above, a tag contains some children (tags inside it) you can extract them into a list. The example would be above table row `<tr></tr>` includes three table data `<td></td>`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a1bb1c",
   "metadata": {},
   "source": [
    "`.findChildren()` will give you alist with all tags inside of a given tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae897d9",
   "metadata": {},
   "source": [
    "You can specify exactly which chhildre, if you want, like with the `.find()`. So you could use \n",
    "* `.findChildren(\"tr\")` or\n",
    "* `.findChildren(class=\"warning_message\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "334efab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete tag code:  <td>400</td> Just the text in the tag:  400\n",
      "Complete tag code:  <td>500</td> Just the text in the tag:  500\n",
      "Complete tag code:  <td>600</td> Just the text in the tag:  600\n"
     ]
    }
   ],
   "source": [
    "middle_row = soup.find(id='middle_row')\n",
    "cells_in_the_row = middle_row.findChildren()\n",
    "for cell in cells_in_the_row:\n",
    "    print('Complete tag code: ', cell, \"Just the text in the tag: \", cell.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05356a0",
   "metadata": {},
   "source": [
    "You can dive deeper into certain tags, for example here you look for all divs from the (CSS) class called hipster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c3834cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whole tag:\n",
      " <div class=\"hipster\">\n",
      "<h2>A Dangerous-Looking Header</h2>\n",
      "<p>\n",
      "I look like a paragraph Kylo Ren could have written.\n",
      "</p>\n",
      "</div> \n",
      "\n",
      "Just the text:  \n",
      "A Dangerous-Looking Header\n",
      "\n",
      "I look like a paragraph Kylo Ren could have written.\n",
      "\n",
      "\n",
      "whole tag:\n",
      " <div class=\"hipster\">\n",
      "<h2>Another Dangerous-Looking Header</h2>\n",
      "<p>\n",
      "This one is not as scary.\n",
      "</p>\n",
      "</div> \n",
      "\n",
      "Just the text:  \n",
      "Another Dangerous-Looking Header\n",
      "\n",
      "This one is not as scary.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_elements = soup.find_all(\"div\", {\"class\" : \"hipster\" })\n",
    "for element in class_elements:\n",
    "    print('whole tag:\\n', str(element), '\\n')\n",
    "    print('Just the text: ', element.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd98c357",
   "metadata": {},
   "source": [
    "Getting all the elements out of the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "770a4504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row: 0\n",
      "whole html: <td>100</td> \tJust content: 100\n",
      "whole html: <td>200</td> \tJust content: 200\n",
      "whole html: <td>300</td> \tJust content: 300\n",
      "Row: 1\n",
      "whole html: <td>400</td> \tJust content: 400\n",
      "whole html: <td>500</td> \tJust content: 500\n",
      "whole html: <td>600</td> \tJust content: 600\n",
      "Row: 2\n",
      "whole html: <td>700</td> \tJust content: 700\n",
      "whole html: <td>800</td> \tJust content: 800\n",
      "whole html: <td>900</td> \tJust content: 900\n"
     ]
    }
   ],
   "source": [
    "# list all tables, since we only have 1, use the first in the list at index 0\n",
    "my_table = soup.find_all('table')[0]\n",
    "# or just use: my_table = soup.find('table')\n",
    "\n",
    "# loop the rows and keep the row number\n",
    "row_num = 0\n",
    "for row in my_table.find_all('tr'):\n",
    "    print(\"Row: \"+str(row_num))\n",
    "    row_num = row_num+1\n",
    "\n",
    "    #loop the cells in the row\n",
    "    for cell in row.find_all('td'):\n",
    "        print(\"whole html:\", str(cell)+\" \\tJust content: \"+cell.text)\n",
    "        \n",
    "# if you'd like, try to change this code to use .findChildren( ) rather t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8738651",
   "metadata": {},
   "source": [
    "## Minitask: Now attempt to scrape something from a real online website:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d2bb3a",
   "metadata": {},
   "source": [
    "Use the above code to make a list of all the degrees available in business school of University of Edinburgh.\n",
    "* You will need to get the source of the page the list is on and feed it into the breautiful soup (see code above). (use this url instead of our demo website file://..... use this: https://www.ed.ac.uk/studying/undergraduate/degrees/index.php?action=view&code=12)\n",
    "* Get the html component that holds all the degrees. Use developer tools to identify what type of component it is (hint: ul stamds for \"unordered list\"). Does this component have a class or an id? How would you get a component when you know it's id? (hint: proxy_degreeList )\n",
    "* What type of a tag are the actual names of degrees in? (div, a, p, or something else) hint: what tag surround the name of the course?\n",
    "* Grab children of that type from the component with all names and in a loop, extract only the text of each of them. And print them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7a8159",
   "metadata": {},
   "source": [
    "I am posting the solution lower down, but do try to solve it by yourself first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "49b56631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy-paste relevant parts of the code from above to start:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2962b7c7",
   "metadata": {},
   "source": [
    "Only uncover the solutions once you tried to complete the task:\n",
    "\n",
    "CLICK HERE TO SEE THE THE HINT 1. \n",
    "1. You will need to get the source of the page the list is on and feed it into the breautiful soup (see code above). (use this url instead of our demo website file://..... use this: https://www.ed.ac.uk/studying/undergraduate/degrees/index.php?action=view&code=12)\n",
    "``` \n",
    "file_url = \"https://www.ed.ac.uk/studying/undergraduate/degrees/index.php?action=view&code=12\" \n",
    "website_source_code = urlopen(file_url) \n",
    "soup_degrees_website = BeautifulSoup(website_source_code, 'html.parser') \n",
    "``` \n",
    "\n",
    "CLICK HERE TO SEE THE THE HINT 2. \n",
    "\n",
    "2. get the html component that holds all the degrees. Use developer tools to identify what type of component it is (hint: ul stamds for \"unordered list\"). \n",
    "Does this component have a class or an id? How would you get a component when you know it's id? (hint: proxy_degreeList )\n",
    "``` \n",
    "degrees = soup_degrees_website.find(id='proxy_degreeList')\n",
    "``` \n",
    "\n",
    "CLICK HERE TO SEE THE THE HINT 3. \n",
    "\n",
    "3. What type of a tag are the actual names of degrees in? (div, a, p, or something else) hint: what tag surround the name of the course? \n",
    "``` \n",
    "for list_item in degrees.findChildren(\"a\"): \n",
    "``` \n",
    "\n",
    "CLICK HERE TO SEE THE THE HINT 4. \n",
    "\n",
    "4. Grab children of that type from the component with all names and in a loop, extract only the text of each of them. And print them. \n",
    "``` \n",
    "print(\"Degree Name:\", list_item.text) \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "60af8034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe9c5fbd",
   "metadata": {},
   "source": [
    "## Scraping reviews using Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a338314b",
   "metadata": {},
   "source": [
    "Here is another example of how Selenium can be used to interact with websites making use of Ajax (Asynchronous JavaScript):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31f97ac",
   "metadata": {},
   "source": [
    "### Selenium is a chrome automation framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2460997e",
   "metadata": {},
   "source": [
    "It will enable us to tell chrome:\n",
    "* go to page bbc.co.uk/weather\n",
    "* \"click the work 'next'\"\n",
    "* scroll down"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed472b55",
   "metadata": {},
   "source": [
    "Selenium will basically open a simplified version of Chrome, for a few seconds, use it and close it afterwards. You might even see it flash on your screen quickly. Then we will use beautiful soup to understand the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9856af",
   "metadata": {},
   "source": [
    "### BeautifulSoup is an HTML parsing framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1d8f51",
   "metadata": {},
   "source": [
    "It will enable us to:\n",
    "* copy the html of the tags eg. div, table\n",
    "* extract text from these tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3b720c",
   "metadata": {},
   "source": [
    "## Getting selenium (don't skip this!)-- You need to download the chromedrive by yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d4aba6",
   "metadata": {},
   "source": [
    "1. find out which version of chrome you have, in chrome open page: chrome://settings/help\n",
    "2. Go to the list of selenium versions and find folder with yoru version (eg. 87.0.4280.88) https://chromedriver.storage.googleapis.com/index.html\n",
    "3. Go into the folder for your version and download the zip file with the version for your operating system (most likely `chromedriver_mac64.zip` or `chromedriver_win32.zip` ).\n",
    "4. unzip that file on yoru machine and put it in the folder where this notebook is. unzipped file will be called `chromedriver` or `chromedriver.exe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c4ebce61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in /Users/caleb/opt/anaconda3/lib/python3.8/site-packages (4.1.0)\r\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /Users/caleb/opt/anaconda3/lib/python3.8/site-packages (from selenium) (0.9.2)\r\n",
      "Requirement already satisfied: urllib3[secure]~=1.26 in /Users/caleb/opt/anaconda3/lib/python3.8/site-packages (from selenium) (1.26.4)\r\n",
      "Requirement already satisfied: trio~=0.17 in /Users/caleb/opt/anaconda3/lib/python3.8/site-packages (from selenium) (0.19.0)\r\n",
      "Requirement already satisfied: idna in /Users/caleb/opt/anaconda3/lib/python3.8/site-packages (from trio~=0.17->selenium) (2.10)\r\n",
      "Requirement already satisfied: outcome in /Users/caleb/opt/anaconda3/lib/python3.8/site-packages (from trio~=0.17->selenium) (1.1.0)\r\n",
      "Requirement already satisfied: async-generator>=1.9 in /Users/caleb/opt/anaconda3/lib/python3.8/site-packages (from trio~=0.17->selenium) (1.10)\r\n",
      "Requirement already satisfied: sortedcontainers in /Users/caleb/opt/anaconda3/lib/python3.8/site-packages (from trio~=0.17->selenium) (2.3.0)\r\n",
      "Requirement already satisfied: sniffio in /Users/caleb/opt/anaconda3/lib/python3.8/site-packages (from trio~=0.17->selenium) (1.2.0)\r\n",
      "Requirement already satisfied: attrs>=19.2.0 in /Users/caleb/opt/anaconda3/lib/python3.8/site-packages (from trio~=0.17->selenium) (20.3.0)\r\n",
      "Requirement already satisfied: wsproto>=0.14 in /Users/caleb/opt/anaconda3/lib/python3.8/site-packages (from trio-websocket~=0.9->selenium) (1.0.0)\r\n",
      "Requirement already satisfied: certifi in /Users/caleb/opt/anaconda3/lib/python3.8/site-packages (from urllib3[secure]~=1.26->selenium) (2021.10.8)\r\n",
      "Requirement already satisfied: pyOpenSSL>=0.14 in /Users/caleb/opt/anaconda3/lib/python3.8/site-packages (from urllib3[secure]~=1.26->selenium) (20.0.1)\r\n",
      "Requirement already satisfied: cryptography>=1.3.4 in /Users/caleb/opt/anaconda3/lib/python3.8/site-packages (from urllib3[secure]~=1.26->selenium) (3.4.7)\r\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/caleb/opt/anaconda3/lib/python3.8/site-packages (from cryptography>=1.3.4->urllib3[secure]~=1.26->selenium) (1.14.5)\r\n",
      "Requirement already satisfied: pycparser in /Users/caleb/opt/anaconda3/lib/python3.8/site-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure]~=1.26->selenium) (2.20)\r\n",
      "Requirement already satisfied: six>=1.5.2 in /Users/caleb/opt/anaconda3/lib/python3.8/site-packages (from pyOpenSSL>=0.14->urllib3[secure]~=1.26->selenium) (1.16.0)\r\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /Users/caleb/opt/anaconda3/lib/python3.8/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.13.0)\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4955486a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3ddc8389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define method that will create a browser, suitable to your operating system\n",
    "import sys\n",
    "def get_a_browser():\n",
    "    if sys.platform.startswith('win32') or sys.platform.startswith('cygwin'):\n",
    "        return webdriver.Chrome() # windows\n",
    "    else:\n",
    "        return webdriver.Chrome('./chromedriver') # mac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f0ea26",
   "metadata": {},
   "source": [
    "**Important Note**: allowing your system to run `chromedriver`. This needs to be done just once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0ceb86",
   "metadata": {},
   "source": [
    "If you are on a mac, you will need to allow your system to use chromium. Run below cell, and you will likely see a warning the first time, click 'cancel' (don't click 'Delete')."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e79a59d",
   "metadata": {},
   "source": [
    "After you see the warning, go into `Settings > Security&Privacy > General` and `\"Allow Anyway\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ef27f5",
   "metadata": {},
   "source": [
    "On a pc the process will be simpler. When asked you'll need to allow computer to use the `chromedriver.exe` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882f3006",
   "metadata": {},
   "source": [
    "## Task: let's try to scrape an interactive website"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6027c1",
   "metadata": {},
   "source": [
    "What will be the weather in Edinburgh in 2 days?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f150ca5a",
   "metadata": {},
   "source": [
    "You need a web browser, pen and paper!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5889ef0",
   "metadata": {},
   "source": [
    "In this task you will be asked to do something by yourself (using your web browser, mouse and keyboard), and then you will see how you cen program `Selenium` to do it for you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4051fe70",
   "metadata": {},
   "source": [
    "**Use www.bbc.co.uk/weather to find out what time will be the sunrise in EDINBURGH next Sunday.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2758b817",
   "metadata": {},
   "source": [
    "Do it at least 3 times and observe all the steps you are taking. Make a very detailed list of all the steps, as if you had to describe them to someone over the phone without seeing their screen. See example below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6c37c5",
   "metadata": {},
   "source": [
    "it will look a bit like this:\n",
    "* ok, go to www.bbc.co.uk/weather and wait for it to load\n",
    "* scroll down, do you see a link with words 'Edinburgh' on it? Click it.\n",
    "* Wait a minute for it to load.\n",
    "* ok, now scroll down and ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efd6021",
   "metadata": {},
   "source": [
    "When you are done with this exercise, we will try to instruct Selenium (Chrome automation tool) to do it for us. Do you think you can try to use Chrome Dev tools to make yoru steps more specific? eg. Instead of saying \"copy text in that bold link next to the word Sunrise\" try to say \"copy text from the html span item with a class `wr-c-astro-data__time`\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ebc270",
   "metadata": {},
   "source": [
    "**SERIOUSLY: Take a few minutes to do this. It will make you learn more from the below code!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01921c07",
   "metadata": {},
   "source": [
    "Ok. And now let's get the python to do it for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "91668503",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-48-36210e268306>:7: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  return webdriver.Chrome('./chromedriver') # mac\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sunrise next Sunday:  08:12\n"
     ]
    }
   ],
   "source": [
    "browser = get_a_browser()\n",
    "\n",
    "# the url we want to open\n",
    "url = u'https://www.bbc.co.uk/weather'\n",
    "\n",
    "# the browser will start and load the webpage\n",
    "browser.get(url)\n",
    "\n",
    "# we wait 1 second to let the page load everything\n",
    "time.sleep(1)\n",
    "\n",
    "# we search for an element that is called 'customer reviews', which is a button\n",
    "# the button can be clicked with the .click() function\n",
    "browser.find_element(By.LINK_TEXT,\"Edinburgh\").click();\n",
    "\n",
    "# sleep again, let everything load\n",
    "time.sleep(1)\n",
    "\n",
    "# we load the HTML body (the main page content without headers, footers, etc.)\n",
    "body = browser.find_element(By.TAG_NAME,'body')\n",
    "\n",
    "# we use seleniums' send_keys() function to physically scroll down where we want to click\n",
    "body.send_keys(Keys.PAGE_DOWN)\n",
    "\n",
    "# search for the next button to access the next reviews\n",
    "try:\n",
    "    # link will look like \"Sun 12Dec\" so we use find_element_by_partial_link_text()\n",
    "    next_button = browser.find_element(By.PARTIAL_LINK_TEXT,'Sun ') \n",
    "    next_button.click()\n",
    "except NoSuchElementException:  #if such element does not exist, just stop looping\n",
    "    print(\"something went wrong. There was no Sunday link.\")\n",
    "    \n",
    "# load current view of the page into a soup\n",
    "soup = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "\n",
    "\"\"\"\n",
    "1. Find all the elements of class pros and print them \n",
    "2. These values include today's sunrise and sunset time, and the following 13 days.\n",
    "3. `browser.page_source` always get the whole page, so we can only find all\n",
    "4. A not smart, but workable solution is to count how many days between today and next sunday \n",
    "   and then choose the right element of all sunrise_tag list.\n",
    "\"\"\"\n",
    "# The whole list\n",
    "sunrise_tag = soup.find_all(\"span\", {\"class\" : 'wr-c-astro-data__time'})\n",
    "# How many days between today and the next sunday\n",
    "diff = int(next_button.get_attribute('id')[-1])\n",
    "\n",
    "print(\"Sunrise next Sunday: \", sunrise_tag[2*diff].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4349ef3",
   "metadata": {},
   "source": [
    "## Using API to access Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ce9fec",
   "metadata": {},
   "source": [
    "Tweepy is a library that interfaces with the Twitter API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6dc8cd5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install tweepy\n",
    "import tweepy\n",
    "# weeeply is a python library for accessing twitter data via twitter API. \n",
    "# # below I am sharing my demo credenmtials, they will work for testing it,\n",
    "# but for your project you'll need to create  your own credentials.\n",
    "# - create a twitter app with your twitter avound (one per group will do) https://developer.twitter.com/en/apps\n",
    "# - follow the tutorial on tweepy to set it up https://tweepy.readthedocs.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eb6e2e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bearer_token = 'AAAAAAAAAAAAAAAAAAAAAMi%2BYAEAAAAA%2F2LLeju%2BgWlNK34g6PMT14scXzQ%3DHa0gE8PJoBnMVlnyoC3648USErcR6E86QadKgbKlBMIrKVNiYz'  # please generate it from twitter developer by yourself and put it here\n",
    "client = tweepy.Client(Bearer_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bf2e3dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @SpeechUnion: We’ve written to @EdinburghUni over its failure to defend academics from a smear that they were part of a “racist gang” wi…\n",
      "RT @SandraNoDuerme: On non-COVID news, I’m so excited to learn that my working paper “The Ideal Race-Typed DEI Worker Image and Its Consequ…\n",
      "Same he is my favorite. Intelligent, ahead of his time. Humble, servant of the people, even though he went to University of Edinburgh. He genuinely loved his people. https://t.co/0LYSvvtF42\n",
      ". @EdinburghUni: SEXUAL VIOLENCE AT THE UNIVERSITY OF EDINBURGH: THE REDRESSAL SYSTEM NEEDS TO CHANGE - Sign the Petition! https://t.co/Huk0KQ0Pmj via @UKChange\n",
      "RT @SpeechUnion: We’ve written to @EdinburghUni over its failure to defend academics from a smear that they were part of a “racist gang” wi…\n",
      "RT @SpeechUnion: We’ve written to @EdinburghUni over its failure to defend academics from a smear that they were part of a “racist gang” wi…\n",
      "RT @SpeechUnion: We’ve written to @EdinburghUni over its failure to defend academics from a smear that they were part of a “racist gang” wi…\n",
      "#actualidad HousinGo by David de Gea firma un convenio con The University of Edinburgh para prácticas de.. https://t.co/8b0j3i15gm\n",
      "#economía_y_negocios HousinGo by David de Gea firma un convenio con The University of Edinburgh para prá.. https://t.co/7HV5YNr1c3\n",
      "@PulpLibrarian Webserver from the University of Edinburgh...\n"
     ]
    }
   ],
   "source": [
    "for tweet in tweepy.Paginator(client.search_recent_tweets, \"University of Edinburgh\",\n",
    "                              max_results=100).flatten(limit=10):\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ddb817",
   "metadata": {},
   "source": [
    "**More details, please refer to https://developer.twitter.com/en/docs/twitter-api/tweets/search/api-reference/get-tweets-search-recent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6254e237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f5fcda2",
   "metadata": {},
   "source": [
    "## Scraping tweets with Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42779d8",
   "metadata": {},
   "source": [
    "In this exercise we will use selenium to copy-paste some tweets straight from the twitter website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290ebeed",
   "metadata": {},
   "source": [
    "Be aware that there are terms and conditions about how you can use these coppied data. If you abuse or overuse scraping, twitter might block or throttle (slow down) your access to their site. (like, don't scrpate 1000s of tweets in 100 parrallel selenium windows)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dca26e",
   "metadata": {},
   "source": [
    "This time, we import selenium first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "af5ee27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e06b89cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define method that will create a browser, suitable to your operating system\n",
    "import sys\n",
    "def get_a_browser():\n",
    "    if sys.platform.startswith('win32') or sys.platform.startswith('cygwin'):\n",
    "        return webdriver.Chrome() # windows\n",
    "    else:\n",
    "        return webdriver.Chrome('./chromedriver') # mac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e80816e",
   "metadata": {},
   "source": [
    "The webdriver object can launch Internet Explorer, Firefox, and Chrome. Despite your preference, the ChromeDriver (which is a light version of Chrome) is the most widely used and complete one. You can use it to start a twitter page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e4bec55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-54-36210e268306>:7: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  return webdriver.Chrome('./chromedriver') # mac\n"
     ]
    }
   ],
   "source": [
    "# launch the browser\n",
    "browser = get_a_browser()\n",
    "\n",
    "# launch the Twitter search page\n",
    "twitter_url = u'https://twitter.com/search?q='\n",
    "\n",
    "# Add the search term\n",
    "query = u'%40edinburgh'\n",
    "# note: %40 is a code for @ symbol, so we're asking for the tweets with @edinburgh\n",
    "\n",
    "# Create the url\n",
    "url = twitter_url+query\n",
    "\n",
    "# Get the page\n",
    "browser.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b54034d",
   "metadata": {},
   "source": [
    "Let's do this again and unleash the power of Selenium by using keyboard controls to manipulate a page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "87f9effa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-54-36210e268306>:7: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  return webdriver.Chrome('./chromedriver') # mac\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tweets scraped:  6\n",
      "\n",
      "--NEXT TWEET---\n",
      " 217 \n",
      "-----\n",
      "\n",
      "\n",
      "--NEXT TWEET---\n",
      " 218 \n",
      "-----\n",
      "\n",
      "\n",
      "--NEXT TWEET---\n",
      " 81 \n",
      "-----\n",
      "\n",
      "\n",
      "--NEXT TWEET---\n",
      " 1 \n",
      "-----\n",
      "\n",
      "\n",
      "--NEXT TWEET---\n",
      " 59 \n",
      "-----\n",
      "\n",
      "\n",
      "--NEXT TWEET---\n",
      " 67 \n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "browser = get_a_browser()\n",
    "browser.get(url)\n",
    "\n",
    "# Let the Tweets load\n",
    "time.sleep(1)\n",
    "\n",
    "# Find the body of the HTML page\n",
    "body = browser.find_element(By.TAG_NAME,'body')\n",
    "\n",
    "# Keep scrolling down using a simulation of the PAGE_DOWN button\n",
    "for _ in range(5):\n",
    "    body.send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(1)\n",
    "    \n",
    "# Get the tweets scores by their class (similar to Beautifulsoup's find())\n",
    "retweets = browser.find_elements(By.XPATH,\"//div[@data-testid='retweet']\");\n",
    "\n",
    "print(\"number of tweets scraped: \", len(retweets))\n",
    "\n",
    "# Print Tweets\n",
    "for retweet in retweets:\n",
    "    print(\"\\n--NEXT TWEET---\\n\", retweet.text, \"\\n-----\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fadcc965",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-f160b508",
   "language": "python",
   "display_name": "PyCharm (Edinburgh)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}